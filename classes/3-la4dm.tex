\input{la4dm.tex}
\usepackage{ragged2e}
\title{Matrix factorization}
\subtitle{Rank and Nullity}
\date{October 13, 2025}

\begin{document}

\frame{\titlepage}
%\section*{Outline:}
\frame{\frametitle{Outline}
\tableofcontents}


\section{System of linear equations}
\frame{\frametitle{System of linear equations}
\begin{definition}\label{lsystem}
The general form of a system of linear equations is
\begin{eqnarray*}
\alpha_{11}x_1+\alpha_{12}x_2+\cdots +\alpha_{1n}x_n&=&\beta_1\\
\alpha_{21}x_1+\alpha_{22}x_2+\cdots +\alpha_{2n}x_n&=&\beta_2\\
\vdots & & \\
\alpha_{m1}x_1+\alpha_{m2}x_2+\cdots +\alpha_{mn}x_n&=&\beta_m
\end{eqnarray*}
where $\alpha_{ij}\ i=1,2,\ldots m,\ j=1,2,\ldots ,n$ and $\beta_i\ i=1,2\ldots ,m$ are given scalars and $x_j\ j=1,2,\ldots ,n$ are the unknowns.
\end{definition}
The system of linear equations is said to be \alert{homogeneous} if $\beta_i=0$ for all $i=1,\ldots ,m,$ otherwise it is called \alert{nonhomogeneous}.

}
\frame{\frametitle{}
Collecting the coefficients in the matrix $\mathbf{A}=[\alpha_{ij}]$ the unknowns and the right-side scalars in the column vectors 
$$\mathbf{x}=\left[\begin{array}{c}
x_1\\
x_2\\
\vdots \\
x_n
\end{array}\right]\quad \mbox{and}\quad \mathbf{b}=\left[\begin{array}{c}
\beta_1\\
\beta_2\\
\vdots \\
\beta_m
\end{array}\right]$$
respectively, the matrix equation $\mathbf{Ax}=\mathbf{b}$ can be obtained.\\
Since the left side of the above matrix equation is a linear combination of the columns of $\mathbf{A}$ the following statement can be obtained:\\
\begin{theorem}
The (\ref{lsystem}) linear system of equations is solvable if and only if $\mathbf{b}$ is the linear combination of columns of the coefficient matrix $\mathbf{A}$, that is, $\mathbf{b}$ is in the subspace spanned by the columns of $\mathbf{A}$.
\end{theorem}

}


\frame{\frametitle{}
\bigskip

A solution of the system of linear equations $\mathbf{Ax}=\mathbf{b}$ is an $n$-tuple (a vector) $\mathbf{s}=(s_1,s_2,\ldots ,s_n),$ such that substituting $s_i\mbox{ for }x_i\ i=1,2,\ldots ,n$ each equation of the system (\ref{lsystem}) becomes true numerical statement. The solution set of (\ref{lsystem}) is the set of all solution vectors.
\bigskip

To provide an effective method for solution of systems of linear equation first we show a process for factoring matrices.
}
\section{Factoring matrices}
\frame{\frametitle{Basis factoring matrices}
Let $\mathbf{A}=[\mathbf{a}_1,\mathbf{a}_2,\ldots ,\mathbf{a}_n]$ be a matrix having rank $r.$ Then there is an $r$ element linearly independent set of vectors $\mathbf{a}_{i_1},\ldots,\mathbf{a}_{i_r}$ such that any vector of the subspace spanned by the columns of $\mathbf{A}$ -- which is called the {\em column space} of $\mathbf{A}$ -- can be expressed as a linear combination of the vectors $\mathbf{a}_{i_1},\ldots,\mathbf{a}_{i_r},$ that is, $\mathbf{a}_{i_1},\ldots,\mathbf{a}_{i_r}$ is a basis of the column space of $\mathbf{A}.$ Therefore
\begin{block}{}
\begin{eqnarray*}
\mathbf{a}_1&=&d_{11}\mathbf{a}_{i_1}+d_{21}\mathbf{a}_{i_2}+\cdots +d_{r1}\mathbf{a}_{i_r}\\
\mathbf{a}_2&=&d_{12}\mathbf{a}_{i_1}+d_{22}\mathbf{a}_{i_2}+\cdots +d_{r2}\mathbf{a}_{i_r}\\
&\vdots& \\
\mathbf{a}_n&=&d_{1n}\mathbf{a}_{i_1}+d_{2n}\mathbf{a}_{i_2}+\cdots +d_{rn}\mathbf{a}_{i_r}.
\end{eqnarray*}
\end{block}
}

\frame{\frametitle{Basis factoring matrices}
Form the matrices 
\begin{block}{}
\[
\mathbf{A}_1=[\mathbf{a}_{i_1},\ldots,\mathbf{a}_{i_r}] \text{ and } 
\mathbf{A}_2=\left[\begin{array}{cccc}
d_{11}&d_{12}&\ldots &d_{1n}\\
d_{21}&d_{22}&\ldots &d_{2n}\\
\vdots \\
d_{r1}&d_{r2}&\ldots &d_{rn}\\
\end{array}\right],
\]
\end{block}
then the above equation can be written in the form: 
\begin{block}{}
$$\mathbf{A}=\mathbf{A}_1\cdot \mathbf{A}_2.$$
\end{block}
Notice that the $j$th column of the matrix $\mathbf{A}_2$ is just the coordinate vector of $\mathbf{a}_j$ with respect to the basis $\{\mathbf{a}_{i_1},\ldots,\mathbf{a}_{i_r}\}.$
}

\frame{\frametitle{}
\begin{example}
Factor the matrix $\mathbf{A}=\left[\begin{array}{rrrr}
1&2&0&3\\
2&1&3&3\\
-1&1&-3&0
\end{array}\right].$
\end{example}

Using elementary basis transformation we find the rank of $\mathbf{A}$ and
a basis of its column space, starting with the standard basis
$\{\mathbf{e}_1,\mathbf{e}_2,\mathbf{e}_3\}$ of $\R^3.$
$$\begin{array}{c|rrrr}
&\mathbf{a}_1&\mathbf{a}_2&\mathbf{a}_3&\mathbf{a}_4\\ \hline
\mathbf{e}_1&\fbox{$1$}&2&0&3\\
\mathbf{e}_2&2&1&3&3\\
\mathbf{e}_3&-1&1&-3&0
\end{array}\longrightarrow \begin{array}{c|rrr}
&\mathbf{a}_2&\mathbf{a}_3&\mathbf{a}_4\\ \hline
\mathbf{a}_1&2&0&3\\
\mathbf{e}_2&\fbox{$-3$}&3&-3\\
\mathbf{e}_3&3&-3&3
\end{array}\longrightarrow \begin{array}{c|rr}
&\mathbf{a}_3&\mathbf{a}_4\\ \hline
\mathbf{a}_1&2&1\\
\mathbf{a}_2&-1&1\\
\mathbf{e}_3&0&0
\end{array}$$
It can be red off from the last table, that the first and second column of $\mathbf{A}$ form a basis of its column space and $\mathbf{a}_3=2\mathbf{a}_1-\mathbf{a}_2$ and $\mathbf{a}_4=\mathbf{a}_1+\mathbf{a}_2.$ 
}
\frame{\frametitle{}
Remember that $j$-th column of the matrix $\mathbf{A_2}$ is the coordinate vector of $\mathbf{a_j}$ with respect to the 
basis $\left\{ \mathbf{a_1},\mathbf{a_2} \right\}$.
Therefore
\begin{exampleblock}{}
$$\mathbf{A}_1=[\mathbf{a}_1,\mathbf{a}_2]=\left[\begin{array}{rr}
1&2\\
2&1\\
-1&1
\end{array}\right]\quad\text{ and }\quad \mathbf{A}_2=\left[\begin{array}{rrrr}
1&0&2&1\\
0&1&-1&1
\end{array}\right],$$
\end{exampleblock}
where we used the obvious fact that $\left[\begin{array}{c}
1\\
0
\end{array}\right]$ and $\left[\begin{array}{c}
0\\
1
\end{array}\right]$ are the coordinate vectors of $\mathbf{a}_1$ respectively $\mathbf{a}_2$ with respect to the basis $\{\mathbf{a}_1,\mathbf{a}_2\}.$

}

\section{The rank theorem}
\begin{frame}{Rank Theorem}
\justifying
Let $A$ be an $m \times n$ matrix, and consider the factorization $A = BC$ as before.\\[0.4em]
The matrix $B$ consists of a maximal linearly independent set of columns of $A$.\\[0.3em]
Because of this maximality, the column span of $B$ coincides with the column space of $A$.\\[0.3em]
Hence, the columns of $B$ form a basis for the column space of $A$.
\pause\\[0.8em]
The matrix $C$ represents the final elimination table obtained after removing all zero rows.\\[0.3em]
The rows of $C$ are linearly independent, since each row contains a pivot position with entry $1$, and all other rows contain $0$ in that position.\\[0.3em]
Therefore, no row can be expressed as a linear combination of the others.\\[0.3em]
Moreover, the row operations used in the elimination process do not change the row space.\\[0.3em]
Consequently, the rows of $C$ form a basis for the row space of $A$.
\\[1em]
From these observations we obtain the following result.
\end{frame}
\frame{
    \begin{theorem}[Rank theorem]
        The dimensions of the column space and the row space of a matrix are equal.
        This common dimension is called the \alert{rank} of the matrix.
        The rank is equal to the number of pivot positions in the Gauss--Jordan elimination of the matrix.
    \end{theorem}
}
\section{The rank--nullity theorem}
\begin{frame}\frametitle{Fundamental Subspaces of a Matrix}
    Assume that a matrix $A\in\mathbb{R}^{m\times n}$ is given.
    This matrix has three important subspaces.
    The \alert{column space} (the \alert{range} of $A$) is a subspace of $\mathbb{R^m}$; 
    the \alert{row space} is the subspace of $\mathbb{R}^n$; 
    and the \alert{kernel} (or null space) of $A$ which is also a subspace of $\mathbb{R}^n$.
    \begin{definition}[Kernel of a Matrix]
        Let $A$ be an $m\times n$ matrix.
        The \alert{kernel} of $A$ is defined as
        \[
            \ker{A}=\left\{ x\in\mathbb{R}^n: Ax=0 \right\}
        \]
        The \alert{nullity} of a matrix is the dimension of its kernel.
    \end{definition}
    Obviously, if the matrix $A$ is considered as the coefficient matrix of a homogeneous system of linear equations,
    then the kernel coincides with the solution set of that system.
\end{frame}
\begin{frame}\frametitle{Rank--Nullity theorem}
    Remember that the rank of a matrix is the dimension of the range,
    which is the same as the number of pivot terms of the elimination tables of the matrix.

    We are going to clarify, that the degree of freedom is the same as the nullity, 
    which is the same as the number of those columns the final 
    elimination table of the matrix which have no pivot terms.

    \begin{theorem}
        The number of columns of a matrix is the sum of the rank and the nullity of the matrix.
    \end{theorem}
\end{frame}
\begin{frame}
    First we should understand a corollary of Steinitz lemma.
    \begin{lemma}
        In a finite-dimensional vector space, every linearly independent set can be extend to a basis.
    \end{lemma}
\begin{proof}
    If the set $\left\{ x_1,\ldots,x_\nu \right\}$ is independent and it is not a spanning set,
    then inserting any vector not in the span of this system we obtain an independent set having $\nu+1$ terms.
    We understand that a linearly independent system can not have more elements than any basis does.
    Repeating this process, in finitely many steps, we obtain larger and larger independent sets
    until the set is a basis.
\end{proof}
\end{frame}
\frame{\frametitle{The proof of the Rank--Nullity theorem}
Assume that $A\in\mathbb{R}^{m\times n}$, and $\left\{ x_1,\ldots,x_\nu \right\} $ 
is a bases for the kernel of $A$.
Insert some vectors to this set expand to a bases for $\mathbb{R}^n$, that is the system 
$\left\{ x_1,\ldots,x_\nu,x_{\nu+1},\ldots,x_n \right\}$ is a bases of $\mathbb{R}^n$.
It is enough to prove that the system $\left\{ Ax_{\nu+1},\ldots,Ax_n \right\}$ is a base of the column space.

Spanning set: Let us assume that $z$ is a vector of the column space, that is $z=Ax$ for some $x$.
But $x=\sum_{j=1}^n\lambda_jx_j$, 
thus 
$z=
A\left(\sum_{j=1}^n\lambda_jx_j  \right)
=
\sum_{j=1}^n\lambda_jA(x_j)
=
\sum_{j=\nu+1}^n\lambda_jA(x_j).
$

Independent set:
Assume that the linear combination
$
\sum_{j=\nu+1}^n\lambda_jA(x_j)=0.
$
It means the vector
$u=\sum_{j=\nu+1}^n\lambda_jx_j$ belongs to the kernel.
Since the system $\{x_1,\ldots,x_\nu\}$ is the base of the kernel there exist
$\alpha_1,\ldots,\alpha_\nu$ such that
\(
    \alpha_1x_1+\ldots+\alpha_\nu x_\nu+0x_{\nu+1}+\ldots+0x_n
    =u=
    0x_1+\ldots+0x_\nu+\lambda_{\nu+1}x_{\nu+1}+\ldots+\lambda_nx_n.
    \)
By linearly independence, the coefficients of the left and right hand side must be the same,
especially
$\lambda_{\nu+1},\ldots,\lambda_n$ must all be zero.
}
\frame{}

\end{document}
The dimensions of the column space and the row space of a matrix are equal.  
This common dimension is called the \textbf{rank} of the matrix.  
The rank is equal to the number of pivot positions in the Gauss--Jordan elimination of the matrix.

\documentclass{beamer}
\usepackage{amsmath, amssymb}
\usepackage{ragged2e}
\justifying


**************************************************************************
\frame{\frametitle{Polytope}
The {\em polytope} is the convex hull of a finite set of vectors can be considered as the generalization of convex polygons and solids like triangle, rectangle, tetrahedron and so forth.

The generalization of the plane is given as follows:
\begin{definition}
Let $\mathbf{n}\in \R^n$ and $\alpha\in \R$ such that $\mathbf{n}\neq \mathbf{0}.$ Then the hyperplane $S(\mathbf{n},\alpha )$ is the set of all vectors $\mathbf{x}\in \R^n$ satisfying the equation $\left\langle \mathbf{n};\mathbf{x}\right\rangle =\alpha .$
\end{definition}

For any two points $\mathbf{x}_1,\mathbf{x}_2\in S(\mathbf{n},\alpha )$ the vector $\mathbf{x}_1-\mathbf{x}_2$ is orthogonal to the vector $\mathbf{n},$ therefore it is often called the normal vector of the hyperplane.

}
